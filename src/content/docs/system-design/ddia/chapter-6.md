---
title: Chapter 6 - 数据分区
description: 数据密集型应用 - 读书笔记
---

数据分区是指将一个大数据集拆分成很多小数据集，每一条数据只属于某个特定的分区。它在不同的系统中有着不同的称呼，但本质上是一样的

* MongoDB, ES, SolorCloud 中称为 Shard
* HBase 中称为 region
* Bigtable 中称为 tablet
* Cassandra 和 Riak 中称为 vnode
* Couchbase 中称为 vBucket

采用数据分区的主要目的是提高可扩展性（Scalability），不同的分区可以放在一个无共享集群的不同节点上。对单 分区进行查询时，每个节点对自己所在分区可以独立执行查询操作，因此添加更多的节点可以提高查询吞吐量。 超大而复杂的查询尽管比较困难，但也可能做到跨节点的并行处理。

## 数据分区与数据复制

分区通常与复制结合使用，即每个分区在多个节点都存有副本。下图展示了主从复制模型与分区组合使用时数据的分布情况，可以总结出以下几点：

* 一个节点上可能存储多个分区
* 每个分区都有自己的主副本，而从副本被分配在其他一些节点来保证高可用
* 一个节点可能既是某些分区的主副本，同时也是其他分区的从副本

![](/images/ddia/chapter-6/shard-with-replica.png)

## 键-值数据的分区

需要解决的关键问题是如何决定哪些记录放在哪些节点上，且保证数据和查询负载均匀分布

### 基于关键字区间的分区

一种分区方式是为每个分区分配一段连续的关键字或者关键字区间范围（以最小值和最大值来指示）。如果知道关键字区间的上下限，就可以轻松地确定哪个分区包含这些关键字。分区边界可以由管理员手动指定，也可以由数据库自动选择，每个分区内可以按照关键字排序保存，这样可以轻松支持区间查询。

**缺点**

某些访问模式会导致热点，例如，关键字是时间戳，则分区对应于一个时间范围。。然而，当测量数据从传感器写入数据库时，所有的写入操作都集中在同一个分区（即当天的分区），这会导致该分区在写入时负载过高，而其他分区始终处于空闲状态。

**解决**

需要使用时间戳以外的其他内容作为关键字的第一项。例如，可以在时间戳前面加上传感器名称作为前缀，这样首先由传感器名称，然后按时间进行分区。假设同时有许多传感器处于活动状态，则写入负载最终会比较均匀地分布在多个节点上

### 基于关键字哈希值的分区

对于上述数据倾斜与热点问题，许多分布式系统采用了基于关键字哈希函数的方式来
分区。一个好的哈希函数可以处理数据倾斜并使其均匀分布，用于数据分区目的的哈希函数不需要在加密方面很强 ，。许多编程语 也有内置的简单哈希函数（主要用于哈希表），但是要注意这些内置的哈希函数可能井不适合分区，例如，`Java` 的 `Object.hashCode`  和 `Ruby` 的 `Object#hash`，同一个键在不同的进程中可能返回
不同的哈希值

一且找到合适的关键字哈希函数，就可以为每个分区分配一个哈希范围（而不是直接
作用于关键字范围），关键字根据其哈希值的范围划分到不同的分区中

![](/images/ddia/chapter-6/hash-shard.png)

这种方总可以很好地将关键字均匀分配到多个分区中。分区边界可以是均匀间隔，
也可以是伪随机选择（在这种情况下，该技术有时被称为一致性哈希）

**缺点**

丧失了良好的区间查询特性，即使关键字相邻，但经过哈希之后会分散在不同的分区中，区间查询就失去了原有的有序相邻的特性。例如，`MongoDB` 如果启用了基于哈希的分片模式，则区间查询会发送到所有的分区上。

**解决**

可以通过组合索引的方式来解决。`Cassandra` 就使用了这种方式来达到一个折中，它的表可以声明为由多个列组成的复合主键。复合主键只有第一部分可用于哈希分区，而其他列则用作组合索引来对 Cassandra SSTable 中的数据进行排序。因此，它不支持在第一列上进行区间查询，但如果为第一列指定好了固定值，可以对其他列执行高效的区间查询

### 负载倾斜与热点

基于哈希的分区方法可以减轻热点但无法避免，一个极端的情况是，所有的 IO 都针对同一个关键字。例如，在社交媒体上某个名人用户有数百万粉丝，当其发布一些热点事件时可能会引发一场访问风暴，出现大量的对相同关键字的写操作。大多数的系统今天仍然无法自动消除这种高度倾斜的负载，而只能通过应用层来减轻倾斜程度。

## 分区与二级索引

之前的讨论都是基于 key-value 数据模型。但是当涉及到二级索引的时候，情况就会变得复杂。二级索引通常不能唯一标识一条记录，而是用来加速特定值的查询。其主要挑战是它们不能规整地映射到分区中。。有两种主要的方法来支持对二级索引进行分区：基于文档的分区和基于词条的分区。

### 基于文档分区的二级索引

如下图所示，一个 NoSQL 数据库存储了汽车信息，并针对 `color` 字段建立了二级索引。在各自的分区中，它们完全独立，各自维护自己的二级索引，且只负责自己分区内的文档而不关心其他分区中的数据。每当需要写数据时，只需要处理包含目标文档 ID 的那个分区，因此文档分区索引也被称为本地索引（local index），而不是全局索引（global index）。

![](/images/ddia/chapter-6/document-based-index.png)

读取时需要注意：除非对文档 ID 做了特殊处理，否则不太可能所有特定颜色或特定品牌的汽车都放在一个分区中（如图所示）。因此如果想要搜索红色汽车，就需要将查询发送到所有的分区，再合并所有返回的结果。这种查询分区数据库的方陆有时也称为分散／聚集（scatter / gather），显然这种二级索引的查询代价高昂。尽管如此，它还是广泛用于实践：MongoDB、Riak、Cassandra、ElasticSearch、SolrCloud、VoltDB 都支持基于文档分区的二级索引。

### 基于词条分区的二级索引

另一种方法，我们可以对所有的数据构建全局索引，而不是每个分区维护自己的本地
索引。而且，为避免成为瓶颈，不能将全局索引存储在一个节点上。所以，全局索引也必须进行分区，且可以与数据关键字采用不同的分区策略。

![](/images/ddia/chapter-6/term-based-index.png)

如上图所示，所有数据分区中的颜色为红色的汽车都被收录到索引 `color:red` 中，而索引本身也是分区的。这种索引方案称为词条分区（term-partitioned），它以待查找的关键字本身作为索引，term 是指文档中出现的所有单词的集合。

**优点**

它的读取更为高效，即它不需要采用 scatter/gather 对所有的分区都执行一遍查询

**缺点**

写入速度较慢且非常复杂，主要因为单个文档的更新时，里面可能会涉及多个二级索引，而二级索引的分区又可能完全不同甚至在不同的节点上，由此势必引人显著的写放大

理想情况下，索引应该时刻保持最新，但是对于词条分区来讲，这需要一个跨多个相关分区的分布式事务支持，写入速度会受到极大的影响，所以现有的数据库都不支持同步更新二级索引。

实践中，对全局二级索引的更新往往都是异步的（也就意味着，如果在写入之后马上去读索引 ，那么刚刚发生的更新可能还没有反映在索引中）。例如，Amazon DynamoDB 的二级索引通常可以在 1 秒之内完成更新，但当底层设施出现故障时，也有可能需要等待很长的时间

## 分区再平衡

随着时间的推移，数据库可能总会出现某些变化来要求数据和请求可以从一个节点转移到另一个节点，这样的一个迁移负载的过程称为再平衡（或者动态平衡）。无论哪种分区方案，分区再平衡通常至少要满足：

* 平衡之后，负载、数据存储、读写请求等应该在集群范围更均匀地分布。
* 再平衡执行过程中，数据库应该可以继续正常提供读写服务。
* 避免不必要的负载迁移，以加快动态再平衡，井尽量减少网络和磁盘 I/O 影响。

### 动态再平衡策略

不能简单地应用取模操作，因为这样会导致大量的节点数据移动。可以考虑以下几种方法：

#### 固定数量的分区

首先，创建远超实际节点数的分区数，然后为每个节点分配多个分区。例如，对于一个 10 节点的集群，数据库可以从一开始就逻辑划分为 1000 个分区，这样大约每个节点承担 100 个分区。如果集群中添加了一个新节点，该新节点可以从现有的节点上匀走几个分区，直到分区再次达到平衡。如果从集群中删除节点，则采取相反的措施

![](/images/ddia/chapter-6/dynamic-rebalance-1.png)


使用该策略时，分区的数量往往在数据库创建时就确定好，之后不会改变。如果数据集的总规模高度不确定或可变（例如，开始非常小，但随着时间的推移可能会变得异常庞大）， 此时如何选择合适的分区数就有些困难。分区大小应该“恰到好处”，如果分区里的数据量非常大，则每次再平衡和节点故障恢复的代价就很大；但是如果每个分区太小，就会产生太多的开销。

#### 动态分区

一些数据库如 HBase 和 RethinkDB 等采用了动态分区。当分区增长超过一个可配的参数阈值，它就拆分成两个分区，每个承担一半的数据量。相反，如果大量数据被删除，并且分区缩小到某个阈值以下，则将其与相邻分区合并（有点类似于 B-Tree）

每个分区总是分配给一个节点，而每个节点可以承载多个分区，这点与固定数量的分
样。当一个大的分区发生分裂之后，可以将其中的一半转移到其他某节点以平衡负载。

动态分区的一个优点是分区数量可以自动适配数据总量。但是，需要注意的是，对于一个空的数据库，因为没有任何先验知识可以帮助确定分区的边界，所以会从一个分区开始。可能数据集很小，但直到达到第一个分裂点之前，所有的写入操作都必须由单个节点来处理，而其他节点则处于空闲状态

#### 按节点比例分区

前面两种策略的分区数量都是与数据集大小成正比，此外还有第三种方式，就是使分区数与集群节点数成正比。换句话说，每个节点具有固定数量的分区，当节点数不变时，每个分区的大小与数据集大小保持正比的关系。当一个新节点加入集群时，它随机选择固定数量的现有分区进行分裂，然后拿走这些分区的一半数据量，将另一半数据留在原节点。

### 自动与手动再平衡操作

全自动式再平衡会更加方便，它在正常维护之外所增加的操作很少。但是，也有可能
出现结果难以预测的情况。出于这样的考虑，让管理员介入到再平衡可能是个更好的选择。它的确比全自动过程响应慢一些，但它可以有效防止意外发生。

## 请求路由

既然我们将数据集分布到了多个节点上，那么当客户端需要发送请求时，它该如何知道应该连接哪个节点？此外，如果发生了分区再平衡，那么分区与节点的关系还会随之变化。这是一类典型的服务发现问题，概括来讲，这个问题有以下几种不同的策略：

* 允许客户端链接任意的节点，如果该节点恰好拥有所请求的分区，则直接处理，否则，将请求转发给下一个合适的节点
* 将所有客户端的请求都发送到一个路由层，由后者负责将请求转发
* 客户端感知分区和节点分配关系

![](/images/ddia/chapter-6/routing.png)

许多分布式数据系统依靠独立的协调服务（如 ZooKeeper ）跟踪集群范围内的元数据，每个节点都向 ZooKeeper 中注册自己， ZooKeeper 维护了分区到节最终映射关系。其他参与者（如路由层或分区感知的客户端 ）可以向ZooKeeper 订阅此信息。一旦分区发生了改变，或者添加、删除节点， ZooKeeper就会主动通知路由层，这样使路由信息保持最新状态（HBase、SolrCloud、Kafka、MongoDB）。

![](/images/ddia/chapter-6/zookeeper.png)

Cassandra 和 Riak 采用不同的方法，它们在节点之间使用 gossip 协议来同步集群状态变化。请求可以发送到任何节点，由该节点负责将其转发到目标分区节点



